<!doctype html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VJF4S0L85"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1VJF4S0L85');
</script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>nanda_transformer | Eric Werner’s Website</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="nanda_transformer" />
<meta name="author" content="Eric Werner" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="‘“Contentment is as rare among men as it is natural among animals. No form of government has ever satisfied its subjects”’ -Caesar and the Christ, Will Durant" />
<meta property="og:description" content="‘“Contentment is as rare among men as it is natural among animals. No form of government has ever satisfied its subjects”’ -Caesar and the Christ, Will Durant" />
<link rel="canonical" href="http://localhost:4000/content/notes/nanda_transformer.html" />
<meta property="og:url" content="http://localhost:4000/content/notes/nanda_transformer.html" />
<meta property="og:site_name" content="Eric Werner’s Website" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="nanda_transformer" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Eric Werner"},"description":"‘“Contentment is as rare among men as it is natural among animals. No form of government has ever satisfied its subjects”’ -Caesar and the Christ, Will Durant","headline":"nanda_transformer","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/index/profile_pic.jpg"},"name":"Eric Werner"},"url":"http://localhost:4000/content/notes/nanda_transformer.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- <link rel="stylesheet" href="/assets/css/style.css"> -->
    <link rel="stylesheet" href="/assets/css/style.css?v=">    
    <link rel="stylesheet" href="/assets/css/custom.css?v=">

</head>
<body>
    <div class="notes-wrapper">
        <header> <h1><a href="http://localhost:4000/">Eric Werner</a></h1>
<img src="/index/profile_pic.jpg" alt="Logo" width="70%" height="70%" /><br>
<!-- <a href="/content/mariana.html">mariana</a><br> -->
<!-- <a href="/content/first_post.html">first post</a><br> -->
<!-- <a href="/content/notes.html">notes</a><br>
<a href="/content/chat_box.html">chat</a><br> -->
resume:<br>
<a href="http://localhost:4000/assets/Eric_Werner_resume.pdf">- Resume</a><br><br>
Machine Learning Projects:<br>
- <a href="/content/ai_projects/cs224r_proj.html">Reinforcement learning for robotic arm</a><br>
- <a href="/content/ai_projects/cs224u_proj.html">LLM Prompt-tuning</a><br>
- <a href="/content/ai_projects/cs229_proj.html">CNN plays GeoGuessr</a><br>
- <a href="/content/ai_projects/cs237a_proj.html">Autonomous Wheeled robot</a><br>
- <a href="/content/ai_projects/cs168_proj.html">Vector Similarity Search</a><br>
<br>
Prior electromechanical projects:<br>
<a href="http://www.ewern.weebly.com" target="_blank">(external site)</a><br> 
            <br><br><br>
            <a style="font-size: 40px;" href="/content/notes.html">&lt;-- notes</a><br>
        </header>
        <section class="notes-layout">
            <h4 id="-what-is-a-transformer"><a href="https://youtu.be/bOYE6E8JrtU" target="_blank">[*]</a> What is a transformer?</h4>

<h3 id="how-a-transformer-block-works-algebraically">How a transformer block works, algebraically</h3>

<ul>
  <li>$A$ attention: “where do I move information from?”</li>
  <li>$x$: residual stream</li>
  <li>$W_v$: values of prompt tokens, “what info do I move to my current position”</li>
  <li>$W_o$: head –&gt; residual stream</li>
  <li>$A,x$ are <strong>activations</strong>, $W_o,W_v$ are <strong>parameters</strong></li>
</ul>

<h3 id="a--x--w_vt--w_ot">$A ~~~ x ~~~ W_v^T ~~~ W_o^T$</h3>

<h3 id="a--x--w_vt--w_ot-1">$(A ~ (x ~ W_v^T)) ~ W_o^T$</h3>

<p>shapes $(p_{dest},p_{source})\times(p_{source},d_{model})\times(d_{model}, d_{head})\times(d_{head},d_{model})$</p>

<p>note on hopeless intution for values:values are kinda meaningless; they are just a low-rank intermediate state in part of a larger $d_{model} \times d_{model}$ matrix $W_{ov} = W_o W_v$.
<!-- residual stream = context vector --></p>

<h5 id="me-trying-to-explain-how-a-transformer-works">me trying to explain how a transformer works</h5>
<ol>
  <li>input text is embedded (most simply, a one-hot vector of length $d_{dictionary}$)
    <ul>
      <li>embedding is a lookup table mapping tokens to vectors</li>
    </ul>
  </li>
  <li>embedded text is run through series of blocks, with a residual stream carrying the word past each layer
    <ul>
      <li>each block runs attention on input, and sends this through an MLP
        <ul>
          <li>context vector: for <em>each</em> of the input tokens, <strong>attention</strong> is run on it’s prior (causal) tokens</li>
          <li>then, the <code class="language-plaintext highlighter-rouge">context_window_length</code> context vectors are passed through MLP, which outputs <code class="language-plaintext highlighter-rouge">softmax(log(logits))</code>
            <ul>
              <li>note that <code class="language-plaintext highlighter-rouge">softmax</code> and <code class="language-plaintext highlighter-rouge">log</code> don’t change dimension, and <code class="language-plaintext highlighter-rouge">logits</code> is a vector with an entry for each token in the dictionary</li>
            </ul>
          </li>
          <li>a 100% confident prediction would be if the MLP output a one-hot vector</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>the <code class="language-plaintext highlighter-rouge">softmax</code>‘ed vectors are sampled to generate a next-token prediction</li>
</ol>

<ul>
  <li>multihead attention: if <code class="language-plaintext highlighter-rouge">d_{embedding} = 512</code> and <code class="language-plaintext highlighter-rouge">n_heads = 8</code>, then each attention head operates in a subspace of size <code class="language-plaintext highlighter-rouge">512/8 = 64</code></li>
</ul>

<h2 id="useful-comments">useful comments</h2>

<p><strong>model attends to context vectors, not tokens</strong>: In mech interp, high activation on a full stop doesn’t mean the full stop was highly relevant; it means that the model compressed info about setence into the full-stop’s residual stream. Keep in mind, each token’s residual stream is a function of all previous tokens, so the model doesn’t need to look at each token individually.</p>

        </section>
    </div>

    <!-- iphone zoom robustness -->
    <!-- <script src="/assets/js/scale.fix.js"></script> -->
    <script>
    
</script>
</body>
</html>