<!doctype html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1VJF4S0L85"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1VJF4S0L85');
</script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>cs224u project | Eric Werner’s Website</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="cs224u project" />
<meta name="author" content="Eric Werner" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="‘“Contentment is as rare among men as it is natural among animals. No form of government has ever satisfied its subjects”’ -Caesar and the Christ, Will Durant" />
<meta property="og:description" content="‘“Contentment is as rare among men as it is natural among animals. No form of government has ever satisfied its subjects”’ -Caesar and the Christ, Will Durant" />
<link rel="canonical" href="http://localhost:4000/content/ai_projects/copy_cs224u_proj.html" />
<meta property="og:url" content="http://localhost:4000/content/ai_projects/copy_cs224u_proj.html" />
<meta property="og:site_name" content="Eric Werner’s Website" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="cs224u project" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Eric Werner"},"description":"‘“Contentment is as rare among men as it is natural among animals. No form of government has ever satisfied its subjects”’ -Caesar and the Christ, Will Durant","headline":"cs224u project","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/index/profile_pic.jpg"},"name":"Eric Werner"},"url":"http://localhost:4000/content/ai_projects/copy_cs224u_proj.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- <link rel="stylesheet" href="/assets/css/style.css"> -->
    <link rel="stylesheet" href="/assets/css/style.css?v=">    
    <link rel="stylesheet" href="/assets/css/custom.css?v=">
</head>
<body>
    
    <div class="wrapper">
        <header> <h1><a href="http://localhost:4000/">Eric Werner</a></h1>
<img src="/index/profile_pic.jpg" alt="Logo" width="70%" height="70%" /><br>
<!-- <a href="/content/mariana.html">mariana</a><br> -->
<!-- <a href="/content/first_post.html">first post</a><br> -->
<!-- <a href="/content/notes.html">notes</a><br>
<a href="/content/chat_box.html">chat</a><br> -->
resume:<br>
<a href="http://localhost:4000/assets/Eric_Werner_resume.pdf">- Resume</a><br><br>
Machine Learning Projects:<br>
- <a href="/content/ai_projects/cs224r_proj.html">Reinforcement learning for robotic arm</a><br>
- <a href="/content/ai_projects/cs224u_proj.html">LLM Prompt-tuning</a><br>
- <a href="/content/ai_projects/cs229_proj.html">CNN plays GeoGuessr</a><br>
- <a href="/content/ai_projects/cs237a_proj.html">Autonomous Wheeled robot</a><br>
- <a href="/content/ai_projects/cs168_proj.html">Vector Similarity Search</a><br>
<br>
Prior electromechanical projects:<br>
<a href="http://www.ewern.weebly.com" target="_blank">(external site)</a><br> </header>
        <section>
            <p><em>Prompt tuning and semantic interpretation with an LLM</em>
<strong>Trained and interpreted soft prompts for language understanding tasks using prompt-tuning, a PEFT (Parameter Efficient Finetuning) method. Improves prompting by revising prompt at an embedding level rather than the english level. Found soft prompt diverges more on harder-to-describe tasks, implying English task description is suboptimal, and prompting on an embedding-level can provide greater accuracy (approaching finetuning accuracy).  Analyzed stochasticity and zero-shot transferability of soft prompts.</strong></p>
<h2 id="notes">Notes</h2>
<p>prompt tuning: simple and effective finetuning method (PEFT method)
	-train prompt parameters for a task instead of giving task description
	-can beat finetuning
testing hypotheses:</p>
<ol>
  <li>explore semantic meaning of soft prompts</li>
  <li>stochasticity of training (Var of accuracy across different train runs)</li>
  <li>zero-shot task transferability
benefits:
    <ul>
      <li>don’t have to think up a perfect prompt. just give it examples</li>
      <li>expands prompting capabilities beyond just English (models think more complexly)</li>
    </ul>
  </li>
</ol>

<font style="color:gray"> Found soft prompt diverges more on harder-to-describe tasks, implying English task description is suboptimal, and prompting on an embedding-level can provide greater accuracy. prompt-tuning is often able to beat finetuning because it let's gradient descent find an optimal prompt. gradient descent talks to the machine on an embedding level that English cannot </font>

<h3 id="methods">methods</h3>
<p>model: <code class="language-plaintext highlighter-rouge">bloomz-560m</code></p>
<h6 id="train-soft-prompt">train soft prompt</h6>
<ol>
  <li>start with description of task (“Does hypothesis entail or neutral about premise?”)</li>
  <li>embed prompt</li>
  <li>optimize prompt on examples
    <h5 id="a-semantic-interpretability">a) semantic interpretability</h5>
    <p>soft-prompt size <code class="language-plaintext highlighter-rouge">(n, 1024)</code> for <code class="language-plaintext highlighter-rouge">n</code> trainable tokens
for each token, find <code class="language-plaintext highlighter-rouge">k</code> nearest tokens (cosine distance) in <code class="language-plaintext highlighter-rouge">bloomz-560m</code>’s dictionary</p>
    <ul>
      <li>results:</li>
    </ul>
    <ul>
      <li>looked at top-3 similar tokens for each soft prompt token</li>
      <li>a few key words didn’t change during training (important to result)</li>
      <li>sometimes a token would be in another language like Chinese</li>
      <li>model likes to be explicitly told to be accurate
        <h5 id="b-stochasticity">b) Stochasticity</h5>
        <p>trained each task 4 times for 50 epochs (<code class="language-plaintext highlighter-rouge">Adam</code> optimizer randomly initializes momentum vectors)
project all 16 <code class="language-plaintext highlighter-rouge">(n,1024)</code> soft prompts into 2D using t-SNE
    - results:</p>
      </li>
      <li>more complicated tasks deviate more from original prompt</li>
      <li>all 4 train runs for each task resulted in equidistant t-SNE distances
        <h5 id="c-zero-shot-task-transferability">c) zero-shot task transferability</h5>
      </li>
    </ul>
  </li>
  <li>subtract prompt embeddings (B-A) to find similarity between tasks (Frobenius norm)</li>
  <li>get accuracy of RTE’s soft prompt on WSC’s validation set
    <ul>
      <li>see if there is a relation between soft-prompt-similarity and task-transferability</li>
      <li>results</li>
    </ul>
    <ul>
      <li>obviously, the more different, the less transferrable</li>
      <li>most soft prompt transfer improved accuracy versus no prompt (90% of transfers)</li>
    </ul>
  </li>
</ol>

<p>prefix tuning: prepends trainable parameters to the hidden states of all transformer blocks</p>

<font style="color:green">Text Color</font>
<font style="color:green">Text Color</font>

<h2 id="takeaways">Takeaways</h2>
<ul>
  <li>understanding inner workings of prompts $\rightarrow$ better design choices</li>
  <li>in 3 of 4 tasks, token for “accurate” appeared in soft prompt, implying model likes to be explicitly told to be accurate</li>
  <li>word “entailed” persisted thru training (important tokens don’t change)</li>
  <li>less semantic meaning than we thought, highlighting interpretability problem</li>
  <li>there are many/infinite soft prompts that get the same accuracy (stochasticity)</li>
  <li>transferability (soft prompts encode more than just task, such as “be accurate”)</li>
</ul>

<h2 id="questions-still">Questions still</h2>
<p>refresh on similarity metrics like cosine</p>

        </section>
    </div>

    <!-- iphone zoom robustness -->
    <!-- <script src="/assets/js/scale.fix.js"></script> -->
    <script>
    
</script>
</body>
</html>