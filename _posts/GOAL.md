## My goal of this blog

My goal is to make existential risk from AI more intuitive to the general population.

i.e. Inner vs Outer alignment
- **ChatGPT** (zero shot): Outer alignment deals with the alignment of the training objective with human values, while inner alignment focuses on the alignment of the trained model's behavior with the intended training objective. Both are crucial for developing safe and ethical AI systems.
- **me:** we use loss functions as a proxy to real goal. We do outer alignment in training, but our goal as humans is inner alignment.




